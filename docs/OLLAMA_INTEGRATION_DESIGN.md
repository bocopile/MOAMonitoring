# Ollama í†µí•© ì„¤ê³„ - MOAO11y Multi-Agent í™•ì¥

## ğŸ“‹ Overview

**ëª©ì **: Ollama ë¡œì»¬ LLMì„ ê¸°ì¡´ Claude/GPT/Gemini ì•„í‚¤í…ì²˜ì— í†µí•©í•˜ì—¬ ë¹„ìš© ìµœì í™” ë° í•˜ì´ë¸Œë¦¬ë“œ Agent êµ¬ì„±

**Ollama í™˜ê²½**:
```bash
# ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸
- llama3 (Meta)
- mistral (Mistral AI)
- phi3 (Microsoft)

# Endpoint
http://192.168.35.245:11434/api/generate
```

---

## ğŸ¯ í†µí•© ì „ëµ

### 1. ê³„ì¸µí™”ëœ ëª¨ë¸ ì„ íƒ ì „ëµ

```
ë¹„ìš© & í’ˆì§ˆ ê³„ì¸µ:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tier 1: Premium (í´ë¼ìš°ë“œ - ë³µì¡í•œ ì‘ì—…)              â”‚
â”‚   - Claude Sonnet 4.5: ì•„í‚¤í…ì²˜ ì„¤ê³„, ë³µì¡í•œ ì½”ë“œ    â”‚
â”‚   - GPT-4o: ë¦¬ë·°, ìµœì í™”                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Tier 2: Standard (í´ë¼ìš°ë“œ - ì¤‘ê°„ ì‘ì—…)              â”‚
â”‚   - Gemini 1.5 Pro: ì¼ë°˜ ê°œë°œ                       â”‚
â”‚   - GPT-4o-mini: í…ŒìŠ¤íŠ¸ ìƒì„±                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Tier 3: Budget (ë¡œì»¬ Ollama - ë‹¨ìˆœ ë°˜ë³µ ì‘ì—…)        â”‚
â”‚   - llama3: ì½”ë“œ í¬ë§·íŒ…, ì£¼ì„ ì¶”ê°€                   â”‚
â”‚   - mistral: ë¬¸ì„œ ìƒì„±, ë²ˆì—­                        â”‚
â”‚   - phi3: ê°„ë‹¨í•œ ë¦¬íŒ©í† ë§, ì´ë¦„ ë³€ê²½                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—ï¸ ì•„í‚¤í…ì²˜ ì„¤ê³„

### ì „ì²´ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ModelRouter                            â”‚
â”‚         (ì‘ì—… ë³µì¡ë„ & ë¹„ìš© ê¸°ë°˜ ëª¨ë¸ ì„ íƒ)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                            â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
     â”‚ CloudProvider â”‚            â”‚ OllamaProviderâ”‚
     â”‚   Manager     â”‚            â”‚   Manager    â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
             â”‚                            â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚        â”‚        â”‚          â”‚        â”‚        â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”    â”Œâ”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”
â”‚Claudeâ”‚ â”‚ GPT â”‚ â”‚Geminiâ”‚   â”‚llama3â”‚ â”‚mistralâ”‚ â”‚ phi3â”‚
â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜
```

### ë””ë ‰í† ë¦¬ êµ¬ì¡° (ì˜ˆìƒ)

```
.claude/scripts/
â”œâ”€â”€ requirements.txt                    # ê¸°ì¡´
â”œâ”€â”€ spec_parser.py                      # ê¸°ì¡´
â”œâ”€â”€ quality_validator.py                # ê¸°ì¡´
â”œâ”€â”€ agent_state.py                      # ê¸°ì¡´
â”‚
â”œâ”€â”€ model_selector.py                   # ìˆ˜ì • í•„ìš”
â”‚   â””â”€â”€ + Ollama ëª¨ë¸ ì •ë³´ ì¶”ê°€
â”‚
â”œâ”€â”€ providers/                          # ì‹ ê·œ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_provider.py               # ì¶”ìƒ í´ë˜ìŠ¤
â”‚   â”œâ”€â”€ anthropic_provider.py          # Claude
â”‚   â”œâ”€â”€ openai_provider.py             # GPT
â”‚   â”œâ”€â”€ google_provider.py             # Gemini
â”‚   â””â”€â”€ ollama_provider.py             # Ollama (ì‹ ê·œ)
â”‚
â”œâ”€â”€ ollama/                             # ì‹ ê·œ Ollama ì „ìš©
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ client.py                      # HTTP í´ë¼ì´ì–¸íŠ¸
â”‚   â”œâ”€â”€ models.py                      # ëª¨ë¸ ì •ì˜
â”‚   â””â”€â”€ streaming.py                   # ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
â”‚
â””â”€â”€ orchestra_hybrid.py                 # ì‹ ê·œ (í•˜ì´ë¸Œë¦¬ë“œ)
```

---

## ğŸ’» í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì„¤ê³„

### 1. OllamaProvider (`providers/ollama_provider.py`)

```python
"""
Ollama Provider - ë¡œì»¬ LLM í†µí•©

íŠ¹ì§•:
- REST API ê¸°ë°˜ í†µì‹ 
- ë¹„ìš©: 0 (ë¡œì»¬)
- ì§€ì—°ì‹œê°„: ë‚®ìŒ (ë¡œì»¬ ë„¤íŠ¸ì›Œí¬)
- í’ˆì§ˆ: ì¤‘í•˜ (ë‹¨ìˆœ ì‘ì—… ì „ìš©)
"""

from typing import Optional, Dict, List
import requests
from .base_provider import BaseProvider

class OllamaProvider(BaseProvider):
    """Ollama ë¡œì»¬ LLM Provider"""

    def __init__(
        self,
        base_url: str = "http://192.168.35.245:11434",
        default_model: str = "llama3",
        timeout: int = 30
    ):
        self.base_url = base_url
        self.default_model = default_model
        self.timeout = timeout

        # ëª¨ë¸ë³„ íŠ¹ì„±
        self.model_specs = {
            'llama3': {
                'strength': ['code_formatting', 'comments', 'simple_refactor'],
                'weakness': ['complex_logic', 'architecture'],
                'speed': 'fast',
                'context_window': 8192
            },
            'mistral': {
                'strength': ['documentation', 'translation', 'summarization'],
                'weakness': ['code_generation', 'debugging'],
                'speed': 'medium',
                'context_window': 32768
            },
            'phi3': {
                'strength': ['variable_naming', 'simple_tasks', 'quick_edits'],
                'weakness': ['complex_code', 'large_context'],
                'speed': 'very_fast',
                'context_window': 4096
            }
        }

    def invoke(
        self,
        prompt: str,
        model: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 2048,
        **kwargs
    ) -> Dict:
        """
        Ollama API í˜¸ì¶œ

        API Endpoint: POST /api/generate
        """
        model = model or self.default_model

        payload = {
            "model": model,
            "prompt": prompt,
            "stream": False,  # ì¼ë‹¨ non-streaming
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens,
            }
        }

        response = requests.post(
            f"{self.base_url}/api/generate",
            json=payload,
            timeout=self.timeout
        )

        if response.status_code != 200:
            raise OllamaAPIError(f"Failed: {response.status_code}")

        result = response.json()

        return {
            'content': result['response'],
            'model': model,
            'done': result['done'],
            'tokens': {
                'input': result.get('prompt_eval_count', 0),
                'output': result.get('eval_count', 0),
                'total': result.get('prompt_eval_count', 0) + result.get('eval_count', 0)
            },
            'cost': 0.0,  # ë¡œì»¬ì´ë¯€ë¡œ ë¹„ìš© ì—†ìŒ
            'latency_ms': result.get('total_duration', 0) / 1_000_000
        }

    def stream(self, prompt: str, model: Optional[str] = None, **kwargs):
        """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (ì‹¤ì‹œê°„ ì¶œë ¥ìš©)"""
        model = model or self.default_model

        payload = {
            "model": model,
            "prompt": prompt,
            "stream": True,
        }

        with requests.post(
            f"{self.base_url}/api/generate",
            json=payload,
            stream=True,
            timeout=self.timeout
        ) as response:
            for line in response.iter_lines():
                if line:
                    yield json.loads(line)

    def is_available(self) -> bool:
        """Ollama ì„œë²„ ìƒíƒœ í™•ì¸"""
        try:
            response = requests.get(
                f"{self.base_url}/api/tags",
                timeout=2
            )
            return response.status_code == 200
        except:
            return False

    def select_best_model(self, task_type: str) -> str:
        """ì‘ì—… ìœ í˜•ì— ë”°ë¼ ìµœì  ëª¨ë¸ ì„ íƒ"""
        task_model_map = {
            'code_formatting': 'llama3',
            'comments': 'llama3',
            'documentation': 'mistral',
            'translation': 'mistral',
            'variable_naming': 'phi3',
            'simple_refactor': 'phi3',
            'summarization': 'mistral',
        }
        return task_model_map.get(task_type, self.default_model)
```

---

### 2. Enhanced ModelSelector (`model_selector.py`)

```python
"""
Enhanced Model Selector - í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì„ íƒ

ì „ëµ:
1. ì‘ì—… ë³µì¡ë„ ë¶„ì„
2. ë¹„ìš© ì œì•½ í™•ì¸
3. Ollama ìš°ì„  (ê°€ëŠ¥í•œ ê²½ìš°)
4. í’ˆì§ˆ ìš”êµ¬ì‚¬í•­ ì¶©ì¡± ì•ˆë˜ë©´ í´ë¼ìš°ë“œë¡œ í´ë°±
"""

class HybridModelSelector:

    # í™•ì¥ëœ ëª¨ë¸ ì •ì˜
    MODEL_CATALOG = {
        # Cloud Models (ê¸°ì¡´)
        'claude-sonnet-4-5': {
            'provider': 'anthropic',
            'cost_input': 3.0,
            'cost_output': 15.0,
            'tier': 'premium',
            'quality': 95,
            'latency': 'medium'
        },
        'gpt-4o': {
            'provider': 'openai',
            'cost_input': 2.5,
            'cost_output': 10.0,
            'tier': 'premium',
            'quality': 90,
            'latency': 'medium'
        },
        'gemini-1.5-pro': {
            'provider': 'google',
            'cost_input': 1.25,
            'cost_output': 5.0,
            'tier': 'standard',
            'quality': 85,
            'latency': 'fast'
        },

        # Local Ollama Models (ì‹ ê·œ)
        'llama3': {
            'provider': 'ollama',
            'cost_input': 0.0,
            'cost_output': 0.0,
            'tier': 'free',
            'quality': 60,  # ë‹¨ìˆœ ì‘ì—…ìš©
            'latency': 'very_fast'
        },
        'mistral': {
            'provider': 'ollama',
            'cost_input': 0.0,
            'cost_output': 0.0,
            'tier': 'free',
            'quality': 65,
            'latency': 'fast'
        },
        'phi3': {
            'provider': 'ollama',
            'cost_input': 0.0,
            'cost_output': 0.0,
            'tier': 'free',
            'quality': 55,
            'latency': 'ultra_fast'
        }
    }

    def select_model(
        self,
        task_type: str,
        task_complexity: str,
        quality_threshold: int = 70,
        prefer_local: bool = True
    ) -> Dict:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì„ íƒ

        ë¡œì§:
        1. Ollamaë¡œ ì²˜ë¦¬ ê°€ëŠ¥? (complexity=simple & quality_threshold ë‚®ìŒ)
           â†’ Ollama ì‚¬ìš© (ë¹„ìš© 0)
        2. ë³µì¡ë„ ë†’ê±°ë‚˜ í’ˆì§ˆ ìš”êµ¬ ë†’ìŒ?
           â†’ í´ë¼ìš°ë“œ ëª¨ë¸ ì‚¬ìš©
        """

        # Simple ì‘ì—…ì´ë©´ì„œ Ollamaë¡œ ì¶©ë¶„í•œ ê²½ìš°
        if task_complexity == 'simple' and prefer_local:
            ollama_model = self._select_ollama_model(task_type)
            if ollama_model and self.ollama_provider.is_available():
                return {
                    'model': ollama_model,
                    'provider': 'ollama',
                    'reason': 'simple_task_local',
                    'cost_estimate': 0.0
                }

        # í’ˆì§ˆ ìš”êµ¬ì‚¬í•­ ì¶©ì¡±í•˜ëŠ” ìµœì € ë¹„ìš© ëª¨ë¸
        candidates = [
            (name, spec) for name, spec in self.MODEL_CATALOG.items()
            if spec['quality'] >= quality_threshold
        ]

        # ë¹„ìš©ìˆœ ì •ë ¬
        candidates.sort(key=lambda x: x[1]['cost_input'])

        selected = candidates[0]

        return {
            'model': selected[0],
            'provider': selected[1]['provider'],
            'reason': 'quality_threshold',
            'cost_estimate': self._estimate_cost(selected[0], 1000)
        }

    def _select_ollama_model(self, task_type: str) -> Optional[str]:
        """ì‘ì—… ìœ í˜•ì— ë”°ë¥¸ Ollama ëª¨ë¸ ì„ íƒ"""
        ollama_tasks = {
            'code_formatting': 'llama3',
            'add_comments': 'llama3',
            'variable_rename': 'phi3',
            'documentation': 'mistral',
            'translation': 'mistral',
            'summarization': 'mistral',
            'simple_refactor': 'phi3',
        }
        return ollama_tasks.get(task_type)
```

---

### 3. LangGraph Hybrid Workflow

```python
"""
orchestra_hybrid.py - Ollama + Cloud í•˜ì´ë¸Œë¦¬ë“œ ì›Œí¬í”Œë¡œìš°

ì›Œí¬í”Œë¡œìš°:
1. ì‘ì—… ë¶„ì„ (Claude) - ë³µì¡ë„ íŒì •
2. Simple â†’ Ollama, Complex â†’ Cloud
3. Ollama ì‹¤íŒ¨ ì‹œ ìë™ í´ë°± â†’ Cloud
"""

def create_hybrid_workflow() -> StateGraph:
    workflow = StateGraph(AgentState)

    # ë…¸ë“œ ì •ì˜
    workflow.add_node("analyze_task", analyze_task_node)        # Claudeê°€ ë³µì¡ë„ ë¶„ì„
    workflow.add_node("generate_ollama", generate_ollama_node)  # Ollamaë¡œ ìƒì„±
    workflow.add_node("generate_cloud", generate_cloud_node)    # Cloudë¡œ ìƒì„±
    workflow.add_node("validate", validate_code_node)           # spec-kit ê²€ì¦
    workflow.add_node("review", review_code_node)               # Claude ë¦¬ë·°
    workflow.add_node("finalize", finalize_node)

    # Entry point
    workflow.set_entry_point("analyze_task")

    # ì¡°ê±´ë¶€ ë¼ìš°íŒ…: ë³µì¡ë„ì— ë”°ë¼ Ollama vs Cloud
    workflow.add_conditional_edges(
        "analyze_task",
        route_by_complexity,
        {
            "ollama": "generate_ollama",    # Simple â†’ Ollama
            "cloud": "generate_cloud"        # Complex â†’ Cloud
        }
    )

    # Ollama ì‹¤íŒ¨ ì‹œ Cloudë¡œ í´ë°±
    workflow.add_conditional_edges(
        "generate_ollama",
        check_ollama_success,
        {
            "success": "validate",
            "fallback": "generate_cloud"     # ì‹¤íŒ¨ â†’ Cloud
        }
    )

    workflow.add_edge("generate_cloud", "validate")
    workflow.add_edge("validate", "review")
    workflow.add_edge("review", "finalize")
    workflow.add_edge("finalize", END)

    return workflow.compile()


def analyze_task_node(state: AgentState) -> AgentState:
    """
    Claudeê°€ ì‘ì—… ë³µì¡ë„ ë¶„ì„

    ì¶œë ¥:
    - complexity: 'simple' | 'medium' | 'complex'
    - recommended_provider: 'ollama' | 'cloud'
    """
    claude = ChatAnthropic(model="claude-sonnet-4-5")

    prompt = f"""ë‹¤ìŒ ì‘ì—…ì˜ ë³µì¡ë„ë¥¼ ë¶„ì„í•´ì¤˜:

ì‘ì—…: {state['original_request']}

ë¶„ì„ ê¸°ì¤€:
- Simple: ë‹¨ìˆœ ë°˜ë³µ, í¬ë§·íŒ…, ì£¼ì„ ì¶”ê°€, ì´ë¦„ ë³€ê²½
- Medium: ì¼ë°˜ ë¡œì§, CRUD, í…ŒìŠ¤íŠ¸ ì‘ì„±
- Complex: ì•„í‚¤í…ì²˜ ì„¤ê³„, ë³µì¡í•œ ì•Œê³ ë¦¬ì¦˜, ìµœì í™”

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{
  "complexity": "simple|medium|complex",
  "reason": "ì´ìœ ",
  "recommended_provider": "ollama|cloud"
}}
"""

    response = claude.invoke(prompt)
    analysis = json.loads(response.content)

    state['task_complexity'] = analysis['complexity']
    state['recommended_provider'] = analysis['recommended_provider']

    return state


def generate_ollama_node(state: AgentState) -> AgentState:
    """Ollamaë¡œ ì½”ë“œ ìƒì„± (Simple ì‘ì—…)"""

    # Ollama provider ì´ˆê¸°í™”
    ollama = OllamaProvider()

    # ì‘ì—… ìœ í˜•ì— ë”°ë¥¸ ëª¨ë¸ ì„ íƒ
    model = ollama.select_best_model(state.get('task_type', 'code_formatting'))

    # ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ (OllamaëŠ” ë³µì¡í•œ ì§€ì‹œ ì´í•´ ì œí•œì )
    prompt = f"""Task: {state['original_request']}

Requirements:
- Java code
- Package: com.moao11y.*
- Follow Google Java Style Guide

Generate the code:
"""

    try:
        response = ollama.invoke(prompt, model=model, temperature=0.3)

        state['current_code'] = response['content']
        state['model_used'] = f"ollama/{model}"
        state['cost'] = 0.0
        state['ollama_success'] = True

        print(f"[OLLAMA] Generated with {model} (cost: $0.00)")

    except Exception as e:
        print(f"[OLLAMA] Failed: {e}")
        state['ollama_success'] = False
        state['error'] = str(e)

    return state


def route_by_complexity(state: AgentState) -> str:
    """ë³µì¡ë„ì— ë”°ë¥¸ ë¼ìš°íŒ…"""
    complexity = state.get('task_complexity', 'complex')

    if complexity == 'simple':
        return "ollama"
    else:
        return "cloud"


def check_ollama_success(state: AgentState) -> str:
    """Ollama ì„±ê³µ ì—¬ë¶€ í™•ì¸"""
    if state.get('ollama_success'):
        return "success"
    else:
        print("[WORKFLOW] Ollama failed, falling back to Cloud")
        return "fallback"
```

---

## ğŸ“Š ì‘ì—…ë³„ ëª¨ë¸ ë§¤í•‘ ì „ëµ

### ì‘ì—… ë¶„ë¥˜ ë° ëª¨ë¸ í• ë‹¹

```yaml
# Simple Tasks â†’ Ollama (ë¹„ìš© 0)
simple_tasks:
  code_formatting:
    model: llama3
    example: "ì½”ë“œ ë“¤ì—¬ì“°ê¸° ë° ìŠ¤íƒ€ì¼ í†µì¼"

  add_comments:
    model: llama3
    example: "ë©”ì„œë“œì— ì£¼ì„ ì¶”ê°€"

  variable_rename:
    model: phi3
    example: "ë³€ìˆ˜ëª…ì„ camelCaseë¡œ ë³€ê²½"

  generate_getter_setter:
    model: phi3
    example: "getter/setter ìë™ ìƒì„±"

  documentation:
    model: mistral
    example: "README ì‘ì„±, JavaDoc ìƒì„±"

  translation:
    model: mistral
    example: "ë¬¸ì„œ í•œì˜ ë²ˆì—­"

# Medium Tasks â†’ Gemini / GPT-4o-mini
medium_tasks:
  crud_implementation:
    model: gemini-1.5-pro

  unit_test_generation:
    model: gpt-4o-mini

  simple_refactoring:
    model: gemini-1.5-pro

# Complex Tasks â†’ Claude / GPT-4o
complex_tasks:
  architecture_design:
    model: claude-sonnet-4-5

  complex_algorithm:
    model: claude-sonnet-4-5

  code_review:
    model: claude-sonnet-4-5

  performance_optimization:
    model: gpt-4o
```

---

## ğŸ”„ Failover & Fallback ì „ëµ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         ì‘ì—… ìš”ì²­                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ë³µì¡ë„ ë¶„ì„ (Claude)                    â”‚
â”‚  - Simple / Medium / Complex             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
        â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
        â–¼           â–¼
    [Simple]    [Complex]
        â”‚           â”‚
        â–¼           â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Ollama  â”‚  â”‚  Cloud  â”‚
  â”‚ (ë¡œì»¬)  â”‚  â”‚ (Claude)â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€[Success]â”€â”€â–º Validation
       â”‚
       â””â”€â”€[Failed]â”€â”€â”€â–º Fallback to Cloud
```

### Fallback ì¡°ê±´
1. **Ollama ì„œë²„ ë‹¤ìš´**: ì¦‰ì‹œ Cloudë¡œ ì „í™˜
2. **ì‘ë‹µ ì‹œê°„ ì´ˆê³¼**: 30ì´ˆ ì´ìƒ â†’ Cloud
3. **í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨**: spec-kit ê²€ì¦ ì‹¤íŒ¨ â†’ Cloudë¡œ ì¬ìƒì„±
4. **ì—ëŸ¬ ë°œìƒ**: 3íšŒ ì¬ì‹œë„ í›„ Cloud

---

## ğŸ’° ë¹„ìš© ìµœì í™” ì‹œë®¬ë ˆì´ì…˜

### ì‹œë‚˜ë¦¬ì˜¤ 1: Ollama ë¯¸ì‚¬ìš© (í˜„ì¬)
```
ì¼ì¼ ì‘ì—… (100ê°œ):
- Simple (30ê°œ) Ã— Gemini Flash = $0.075 Ã— 30 = $2.25
- Medium (50ê°œ) Ã— Gemini Pro = $1.25 Ã— 50 = $62.50
- Complex (20ê°œ) Ã— Claude = $3.00 Ã— 20 = $60.00

Total: $124.75/day
ì›” ë¹„ìš©: $3,742.50
```

### ì‹œë‚˜ë¦¬ì˜¤ 2: Ollama í™œìš© (í•˜ì´ë¸Œë¦¬ë“œ)
```
ì¼ì¼ ì‘ì—… (100ê°œ):
- Simple (30ê°œ) Ã— Ollama = $0.00 Ã— 30 = $0.00 âœ…
- Medium (50ê°œ) Ã— Gemini Pro = $1.25 Ã— 50 = $62.50
- Complex (20ê°œ) Ã— Claude = $3.00 Ã— 20 = $60.00

Total: $122.50/day
ì›” ë¹„ìš©: $3,675.00

ì ˆê°ì•¡: $67.50/ì›” (1.8% ì ˆê°)
```

### ì‹œë‚˜ë¦¬ì˜¤ 3: Ollama ì ê·¹ í™œìš©
```
ì¼ì¼ ì‘ì—… (100ê°œ):
- Simple (30ê°œ) Ã— Ollama = $0.00 âœ…
- Medium (50ê°œ):
  - 30ê°œ Ã— Ollama (ë‹¨ìˆœí•œ ê²ƒë“¤) = $0.00 âœ…
  - 20ê°œ Ã— Gemini = $25.00
- Complex (20ê°œ) Ã— Claude = $60.00

Total: $85.00/day
ì›” ë¹„ìš©: $2,550.00

ì ˆê°ì•¡: $1,192.50/ì›” (31.8% ì ˆê°) ğŸ¯
```

---

## ğŸš€ êµ¬í˜„ ë¡œë“œë§µ

### Phase 1: Ollama Provider êµ¬í˜„ (1ì£¼)
```
1. OllamaProvider í´ë˜ìŠ¤
   - HTTP í´ë¼ì´ì–¸íŠ¸
   - ëª¨ë¸ ê´€ë¦¬
   - ì—ëŸ¬ í•¸ë“¤ë§

2. í…ŒìŠ¤íŠ¸
   - ì—°ê²° í…ŒìŠ¤íŠ¸
   - ëª¨ë¸ë³„ ì„±ëŠ¥ ì¸¡ì •
   - ì‹¤íŒ¨ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
```

### Phase 2: ModelSelector í™•ì¥ (1ì£¼)
```
1. HybridModelSelector êµ¬í˜„
   - Ollama ëª¨ë¸ ì¶”ê°€
   - ë³µì¡ë„ ê¸°ë°˜ ì„ íƒ ë¡œì§
   - Fallback ë©”ì»¤ë‹ˆì¦˜

2. ë¹„ìš© ì¶”ì 
   - Ollama vs Cloud ì‚¬ìš©ëŸ‰
   - ì ˆê°ì•¡ ê³„ì‚°
```

### Phase 3: LangGraph í†µí•© (1ì£¼)
```
1. orchestra_hybrid.py
   - analyze_task_node
   - generate_ollama_node
   - Conditional routing

2. í†µí•© í…ŒìŠ¤íŠ¸
   - Simple ì‘ì—… â†’ Ollama
   - Complex ì‘ì—… â†’ Cloud
   - Failover ì‹œë‚˜ë¦¬ì˜¤
```

### Phase 4: í”„ë¡œë•ì…˜ ì¤€ë¹„ (1ì£¼)
```
1. ëª¨ë‹ˆí„°ë§
   - ì„±ê³µë¥  ì¶”ì 
   - ì‘ë‹µ ì‹œê°„ ì¸¡ì •
   - ë¹„ìš© ì ˆê° ë¦¬í¬íŠ¸

2. ë¬¸ì„œí™”
   - Ollama ì„¤ì • ê°€ì´ë“œ
   - ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ
   - íŠ¸ëŸ¬ë¸”ìŠˆíŒ…
```

---

## âš ï¸ ì£¼ì˜ì‚¬í•­ ë° ì œì•½

### Ollamaì˜ í•œê³„
1. **í’ˆì§ˆ**: í´ë¼ìš°ë“œ ëª¨ë¸ë³´ë‹¤ ë‚®ìŒ
   - ë³µì¡í•œ ë¡œì§ ìƒì„± ë¶€ì í•©
   - ì•„í‚¤í…ì²˜ ì„¤ê³„ ë¶ˆê°€ëŠ¥

2. **ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°**: ì œí•œì 
   - llama3: 8K tokens
   - phi3: 4K tokens (ë§¤ìš° ì‘ìŒ)
   - í° íŒŒì¼ ì²˜ë¦¬ ë¶ˆê°€

3. **ì•ˆì •ì„±**: ë¡œì»¬ ì„œë²„ ì˜ì¡´
   - ì„œë²„ ë‹¤ìš´ ì‹œ ì„œë¹„ìŠ¤ ì¤‘ë‹¨
   - ë„¤íŠ¸ì›Œí¬ ì´ìŠˆ ì˜í–¥

### ê¶Œì¥ ì‚¬ìš© íŒ¨í„´
```python
# âœ… ì¢‹ì€ ì‚¬ìš© (Simple ì‘ì—…)
tasks_for_ollama = [
    "Add comments to this method",
    "Format this code to Google Style",
    "Rename variables to camelCase",
    "Generate getter/setter",
    "Write JavaDoc for this class",
]

# âŒ ë‚˜ìœ ì‚¬ìš© (Complex ì‘ì—…)
tasks_not_for_ollama = [
    "Design microservice architecture",
    "Optimize this algorithm",
    "Review security vulnerabilities",
    "Refactor entire module",
    "Generate complex business logic",
]
```

---

## ğŸ“ ì„¤ì • íŒŒì¼ ì˜ˆì‹œ

### `.env` í™•ì¥
```bash
# Ollama ì„¤ì •
OLLAMA_BASE_URL=http://192.168.35.245:11434
OLLAMA_DEFAULT_MODEL=llama3
OLLAMA_TIMEOUT=30
OLLAMA_ENABLED=true

# Failover ì„¤ì •
OLLAMA_FALLBACK_ENABLED=true
OLLAMA_MAX_RETRIES=3

# ë¹„ìš© ìµœì í™”
PREFER_LOCAL_MODELS=true
QUALITY_THRESHOLD=70  # 70ì  ì´ìƒ í•„ìš”í•˜ë©´ Cloud
```

### `requirements.txt` ì¶”ê°€
```python
# ê¸°ì¡´ ì˜ì¡´ì„± ìœ ì§€...

# Ollama í†µí•© (ì¶”ê°€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆí•„ìš” - requestsë§Œ ì‚¬ìš©)
# requests==2.32.3  # ì´ë¯¸ ìˆìŒ
```

---

## ğŸ¯ ê²°ë¡ 

### ì¶”ì²œ ì ‘ê·¼ë²•

**ë‹¨ê³„ë³„ ë„ì…**:
1. âœ… **Phase 1**: Simple ì‘ì—…ë§Œ Ollama (ìœ„í—˜ ë‚®ìŒ)
2. âœ… **Phase 2**: ì„±ê³µë¥  80% ì´ìƒì´ë©´ Medium ì¼ë¶€ë„ Ollama
3. âš ï¸ **Phase 3**: ComplexëŠ” í•­ìƒ Cloud (í’ˆì§ˆ ë³´ì¥)

**í•µì‹¬ ì›ì¹™**:
```
"Ollama First, Cloud Fallback"

1. ëª¨ë“  ì‘ì—…ì„ ì¼ë‹¨ Ollamaë¡œ ì‹œë„
2. ì‹¤íŒ¨ ì‹œ ìë™ìœ¼ë¡œ Cloudë¡œ í´ë°±
3. ë¹„ìš© 0ìœ¼ë¡œ ìµœëŒ€í•œ ì²˜ë¦¬
4. í’ˆì§ˆ ìš”êµ¬ì‚¬í•­ì€ ì ˆëŒ€ íƒ€í˜‘ ì•ˆí•¨
```

### ê¸°ëŒ€ íš¨ê³¼
- ğŸ’° ë¹„ìš©: 20-30% ì ˆê°
- ğŸš€ ì†ë„: Simple ì‘ì—… 2-3ë°° ë¹ ë¦„ (ë¡œì»¬)
- ğŸ”’ ë³´ì•ˆ: ë¯¼ê° ì •ë³´ ë¡œì»¬ ì²˜ë¦¬
- ğŸ“ˆ í™•ì¥ì„±: ë¡œì»¬ GPU ì¶”ê°€ë¡œ ë¬´í•œ í™•ì¥

---

**ì„¤ê³„ ì‘ì„±ì¼**: 2025-11-05
**ì‹¤ì œ êµ¬í˜„ ì‹œì‘**: Sprint 2 ì´í›„ ê¶Œì¥
**ì˜ˆìƒ êµ¬í˜„ ê¸°ê°„**: 4ì£¼
